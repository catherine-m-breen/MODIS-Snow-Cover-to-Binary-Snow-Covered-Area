# -*- coding: utf-8 -*-
"""CEWA568_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlChYvS8aZfyMJmUg9nphKntCo2WMoC3

January 12, 2021

### Data Processing 
Code to analyze MODIS NDSI and Snow detected from Camera Trap images. 

1) This code should clean up the CSV files. Clean up should include: 
- Only make it for dates and time of interest (Images between October 15 and May 15). 
- Only look at the cameras at 8:00AM (make sure to check for time zone) 
- Make one dataframe (write this as a function) 

2) Load MODIS NDSI table
3) Load remote sensing covariate table: tree cover, land class(? -- might be correlated with tree cover), and NDVI (? -- also might be correlated with tree cover), slope, and cloud cover

4) Merge all covariates, NDSI, and Camera Trap snow cover values

### Analysis

5) Run correlation, regression, and ANOVA -- General Linear Model or ANOVA? 
- RESPONSE VARIABLES: Snow cover from camera traps and MODIS satellite
- COVARIATES: see above, but tree cover, land class (?), NDVI (?), slope, cloud cover, temperature (?), time of day(?). We should probably run all of them but my hunch is that cloud cover and tree cover will probably be the most predictive for agreement/ disagreement. 
- Run correlation for all categories (ordinal), binary 0,1 (at 1 or greater), and binary 0,1, (at 2 or greater)

6) Use thresholds to adjust MODIS maps and see the range in difference in snow covered area for one landsat scene with a bunch of cameras? 
  Do it for MODIS? NOT landsat
  - Do it for one MODIS scene and see the range in SCA for MODIS scene for one day (in February or whatever)
  - see if it increases are decreases the accuracy for MODIS 
  Note: this is in Google Earth Engine, not in python. 
  - Visualization -- not necessary (except for maybe a poster or a presentation, so could have one handy and then save it as a figure) . The output will be the range in snow covered area. From XX% to XX%. The one using the Appel and Salmonson product. And then a correlation between landsat scene at modis with 1) Appel and Salomonson 2) lower end of SCA and 3) upper end of SCA. Maybe just do 1 landsat scene. Say that there is a caveat that this could be done at multiple times of year, and that more work is needed to understand how these different thresholds can be incorporated into one another.
"""

import pandas as pd
import glob, os
from google.colab import drive
from pandas import datetime
drive.mount('/content/drive')

pip install git+https://github.com/statsmodels/statsmodels

import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import logit

import numpy as np
import pandas as pd
import scipy.stats as stats

from statsmodels.miscmodels.ordinal_model import OrderedModel

## upload all the camera data & Remote Sensing Data
path = r'drive/My Drive/SnowAndWeatherLabeling/Processed_ddbs_and_CSVs/CSVs'                     # use your path
all_files = glob.glob(os.path.join(path, "*.csv")) 
rawCameraData = pd.concat(map(pd.read_csv, all_files)) ## pd.concat is bad at datetime so you have to save it and then 
rawCameraData.to_csv('drive/My Drive/CBRE/Analysis/labels.csv')

rawCameraData = pd.read_csv('drive/My Drive/CBRE/Analysis/fullData_glob2.csv')
RemoteSensingData = pd.read_csv('drive/My Drive/CBRE/Analysis/MODIS_allCameras_Jan18-Apr20.csv')
## maybe try it with datesearchMODIS.csv ## which is the file that I used on the original match
RemoteSensingData_nonNull = pd.read_csv('drive/My Drive/CBRE/Analysis/dateSearchMODIS.csv')

#pd.read_csv('drive/My Drive/CBRE/Analysis/MODIS_allCameras_Oct19-Apr20.csv')

columnNames = ['File', 'Folder', 'RelativePath', 'Date', 'Time',
       'Analyst', 'PoorQualilty', 'Temperature', 'SnowCover', 'Weather',
       'SecondOpinion', 'Highlight', 'People', 'Comment', 'DeleteFlag',
       'ImageQuality', 'SurfaceShimmer']

"""There is an issue with the time change where basically when the initial match for the image labeling was done, it was looking at the images in the US time zone (-9hour behind Oslo) rather than the Oslo time zone. So the image in Oslo would say 8:00AM, but in the US it would say that it was 11:00PM. Therefore some of the images are off by a day. To account for this, there is a time zone adjusted value to do the join. The alternative would be to RESORT all the images from the initial script, resetting the time zone, which is sort of tricky to do because the computer is reading the meta data. The error is only for 8:00AM photos. So all images are still within one day of each other. """

## preview the RemoteSensingData with all the nulls removed. 
## Look at an example camera (i.e. 1158), and see what the date is. Then 
## check what the ACTUAL date is on the camera photo
## For example, MODIS has a value for Jan 6 18, and the image when read in the US was 
## at 11PM at night, but it was actually taken at 8AM Norway time, with a date of 
## 1/7/18. So if the timelapse is at 8AM, a day should be subtracted from the 
## the date or the MODIS. This is considered marginal because MODIS is taken at various 
## times during the day, and the time zone is recorded in GMT time zone, which 
## is -2hours from Norway time. Therefore, at most the images could be different in time by 
## 30 hours or as little as 6 hours. 

RemoteSensingData_nonNull.head()
RemoteSensingData_nonNull.loc[RemoteSensingData_nonNull["LokalitetID"] == int('1158')][0:15]

print(len(RemoteSensingData))
z = RemoteSensingData['mean'][1] ## looking at nulls

a = np.array(RemoteSensingData['mean'])
a[~np.isnan(a)]
len(a[~np.isnan(a)])

392453/1703702 ## 23% is not-null

### function to clean up the camera trap images
##### Dataframe Processing for Camera Traps: 
    #Filterimages for DeleteFlag and ImageQuality
    #convert date to date/time object                X
    ## filter for Time to only 8:00AM & 0:00AM       X
    # set date as index
    # pull out the Lokalitet ID                      X
    # test (8/2): reset index                        X note: this is because we want it to be its own data frame
    # check if there are missing dates and insert a NaN??
    # drop duplicates in the practice folder -- keep only Katie Breen labeled 

def cameraLabelingCleanUp(df):
  df = df[columnNames] ## to clean up the index columns
  rowsToDrop = df[df['Folder'] == '2018_practice'][df['Analyst'] != 'Katie Breen'] ## keep only Katie Breen for the practice
  df = df.drop(rowsToDrop.index)
  df = df.drop_duplicates(subset=['File'], keep='last') ## drop any remaining duplicates
  df['Date'] = pd.to_datetime(df['Date']) 
  CameraID = df["File"].str.split("_", n = 1, expand = True) 
  df['LokalitetID'] = CameraID[0].astype(int)
  SnowCover = np.array(df['SnowCover'])
  df = df[~np.isnan(SnowCover)]
  #clean1 = df[df['Time'] == '8:00:00']
  #clean2 = df[df['Time'] == '0:00:00']
  #clean3 = df[df['Time'] == '08:00:00']
  #df1 = clean1.append(clean2)
  #df = df1.append(clean3)
  df = df.reset_index(drop=True) ## test as of 8/2 ## needs to be last for the index stuff
  return df
# df = df['Date'] = pd.to_datetime(df['Date'])

## function to adjust raw data to time change 

def timechange(df):
  ### initial clean up
  df = df[columnNames] ## to clean up the index columns
  rowsToDrop = df[df['Folder'] == '2018_practice'][df['Analyst'] != 'Katie Breen'] ## keep only Katie Breen for the practice
  df = df.drop(rowsToDrop.index)
  df = df.drop_duplicates(subset=['File'], keep='last') ## drop any remaining duplicates
  df['Date'] = pd.to_datetime(df['Date']) 
  CameraID = df["File"].str.split("_", n = 1, expand = True) 
  df['LokalitetID'] = CameraID[0].astype(int)
  SnowCover = np.array(df['SnowCover'])
  df = df[~np.isnan(SnowCover)]
  #clean1 = df[df['Time'] == '8:00:00']
  #clean2 = df[df['Time'] == '0:00:00']
  #clean3 = df[df['Time'] == '08:00:00']
  #df1 = clean1.append(clean2)
  #df = df1.append(clean3)
  df = df.reset_index(drop=True) 

### adjust for time change; WA and Oslo are 9 hours apart
  df['Date_timeChange'] = pd.Timestamp('2000-01-01 00:00:00') ## dummy columns
  for j in range(0,len(df)):
    if pd.to_datetime(df['Time'][j]).hour <= 9:
  #(CameraData['Time'][j] == '0:00:00') or (CameraData['Time'][j] == '08:00:00') or (CameraData['Time'][j] == '8:00:00'):
      df['Date_timeChange'][j] = df['Date'][j] - pd.Timedelta(value = 1, unit = 'day')
    else: df['Date_timeChange'][j] = df['Date'][j] 

  return df

## df should be cleanedData that is either time adjusted or not!! 
def timelapseOnly(df):
  clean1 = df[df['Time'] == '8:00:00']
  clean2 = df[df['Time'] == '0:00:00']
  clean3 = df[df['Time'] == '08:00:00']
  df1 = clean1.append(clean2)
  df = df1.append(clean3)
  df = df.reset_index(drop=True) 
  return df

def timelapseAndOthers(cleanedData):
  df = cleanedData
  ## filter them to switch the order
  cleanTimelapse = df[(df['Time'] == '8:00:00') | (df['Time'] == '0:00:00') | (df['Time'] == '08:00:00')]
  cleanNonTimelapse = df[(df['Time'] != '8:00:00') & (df['Time'] != '0:00:00') & (df['Time'] != '08:00:00')] ## all the lines that aren't timelapse
  ### this time add the filtered one on top
  df = cleanTimelapse.append(cleanNonTimelapse)  
  df = df.reset_index(drop=True)
  ## now drop all but the first occurence
  df = df.drop_duplicates(subset=['LokalitetID','Date'], keep='first')

  return df

### function to clean up MODIS data
  # Filter for Camera of interest                 X
  # min and max dates to match camera traps       X
  # set date as index                             X
  # divide MODIS values by zero                   X
  # rename column to NDSI bc mean bugs it         X
  # the timestamp occurs at 2019-10-01 00:00:00 so convert it to datetime
  # drop na values for MODIS

def MODISSnowCoverCleanUp(df):
  df['date'] = pd.to_datetime(df['date'].astype(str)) #.astype(int) #, format = '%y-%m-%d')
  df.rename(columns={'mean':'NDSImodis'}, inplace=True)
  a = np.array(RemoteSensingData['NDSImodis'])
  df = df[~np.isnan(a)] ## dropping nas
  df = df.reset_index(drop=True) ## test as of 8/2
  return df

RSdata = MODISSnowCoverCleanUp(df=RemoteSensingData)
print(len(RSdata))

print(RSdata['date'][1])
#print(CameraData['Date'][1])

## covariates (could add more)
treecanopyRaw = pd.read_csv('drive/My Drive/CBRE/Analysis/2015treecanopy_allCameras.csv') ## look up resolution
landcoverRaw = pd.read_csv('drive/My Drive/CBRE/Analysis/MODISlandcover2020_allCameras.csv')
NDVImodisRaw = pd.read_csv('drive/My Drive/CBRE/Analysis/NDVIModisdaily.csv')

### functions to clean up covariates
## feed in the covariates of interest

def landcoverClean(landcover):
#### landcover
  landcover.rename(columns={'mode':'landclass'}, inplace=True)
  landcover = landcover[['LokalitetID','landclass']]
  return landcover

def treecanopyClean(treecanopy):
### treecanopy
  treecanopy.rename(columns={'mean':'treecanopyheight'}, inplace=True)
  treecanopy = treecanopy[['LokalitetID','treecanopyheight']] 
  return treecanopy

def NDVImodisClean(NDVImodis):
### NDVImodis
  NDVImodis['date'] = pd.to_datetime(NDVImodis['date'])
  NDVImodis.rename(columns={'mean':'NDVImodis'}, inplace=True)
  NDVImodis = NDVImodis[['LokalitetID','date','NDVImodis']]
  return NDVImodis

landcover = landcoverClean(landcover=landcoverRaw)
treecanopy = treecanopyClean(treecanopy=treecanopyRaw)
NDVImodis = NDVImodisClean(NDVImodis = NDVImodisRaw)

#print(landcover.head())
#print(treecanopy.head())
#print(NDVImodis.head())

def mergeCovariates(landcover, treecanopy, NDVImodis):
  result = pd.merge(treecanopy, landcover, on = 'LokalitetID', how ='outer')
  covariates = pd.merge(NDVImodis,result, on = 'LokalitetID', how='outer')
  return covariates

covariates = mergeCovariates(landcover, treecanopy, NDVImodis)

"""Now that all the data is imported and cleaned up (steps 1-3), we will now merge it all into one dataframe (step 4)

"""

## Step 4
## merge the data (CameraData, RSdata, covariates)

def allDataMerge(CameraData, RSdata, covariates, dateColumn):
  #data = RSdata.merge(covariates,  how='inner', left_on = ['LokalitetID','date'], right_on=['LokalitetID','date'])
 # CameraData = CameraData.set_index(CameraData['Date'])
  #data = data.set_index(RSdata['date'])
  #CameraData = CameraData.set_index(CameraData['Date'])
  data = CameraData.merge(RSdata,  how='inner', left_on = ['LokalitetID', dateColumn], right_on=['LokalitetID','date'])
  ## keep only columns of interest
  return data

######### Run the tree and the merge on rawCameraData#### 
## here are all the functions listed now 

# Tree1: cameraLabelingCleanUp(df)
# Tree2: timechange(df):

# timelapseOnly(df):
# timelapseAndOthers(cleanedData):

# def allDataMerge(CameraData, RSdata, covariates, dateColumn):

############################################################

##Tree 1: no time zone change
CameraData = cameraLabelingCleanUp(df=rawCameraData)
  #A: time-lapse only 
CameraData_timelapse1 = timelapseOnly(df = CameraData)
print(len(CameraData_timelapse1))
  #B: timelapse and others
CameraData_timelapseAndOthers1 = timelapseAndOthers(cleanedData = CameraData)
print(len(CameraData_timelapseAndOthers1))

##Tree 2: time zone change
CameraData_timeChange = timechange(df=rawCameraData)
  #A: time-lapse only 
CameraData_timelapse2 = timelapseOnly(df = CameraData_timeChange)
print(len(CameraData_timelapse2))
  #B: timelapse and others
CameraData_timelapseAndOthers2 = timelapseAndOthers(cleanedData = CameraData_timeChange)
print(len(CameraData_timelapseAndOthers2))

#### All 4 merges 
CameraData_timelapse1_merge = allDataMerge(CameraData_timelapse1, RSdata, covariates, dateColumn = 'Date') # 2358
CameraData_timelapseAndOthers1_merge = allDataMerge(CameraData_timelapseAndOthers1, RSdata, covariates, dateColumn = 'Date') #2621

CameraData_timelapse2_merge = allDataMerge(CameraData_timelapse2, RSdata, covariates, dateColumn = 'Date_timeChange') # 1273
CameraData_timelapseAndOthers2_merge = allDataMerge(CameraData_timelapseAndOthers2, RSdata, covariates, dateColumn = 'Date_timeChange') # 1362


#CameraData_timelapse1_merge.to_csv('drive/My Drive/CBRE/Analysis/CameraData_timelapse1_merge.csv')
#CameraData_timelapseAndOthers1_merge.to_csv('drive/My Drive/CBRE/Analysis/CameraData_timelapseAndOthers1_merge.csv')
#CameraData_timelapse2_merge.to_csv('drive/My Drive/CBRE/Analysis/CameraData_timelapse2_merge.csv')
#CameraData_timelapseAndOthers2_merge.to_csv('drive/My Drive/CBRE/Analysis/CameraData_timelapseAndOthers2_merge.csv')

### 
def sampleSize(df1 , df2, df3, df4):
  df = df1.append(df2).append(df3).append(df4)
  data = df.drop(columns=['Date_timeChange'])
  data = data.drop_duplicates(subset=['LokalitetID','date']) ## could make this fancier 
  data = data.reset_index(drop=True)
  ## data['File'].unique()
  return data

data = sampleSize(CameraData_timelapse1_merge, CameraData_timelapseAndOthers1_merge, CameraData_timelapse2_merge, CameraData_timelapseAndOthers2_merge)
print(data)
#length should be 3637



#print(RSdata[RSdata['LokalitetID'] == int('1119')][RSdata['date'] == '2019-01-31'])
#print(RemoteSensingData[RemoteSensingData['LokalitetID'] == int('1119')][RemoteSensingData['date'] == '2019-01-31'])
## TAKEAWAY: the merge is working, but there the nulls weren't filtered out ahead of sorting all the photos
## so some photos were sorted, but there were NaN values in the table 
## Weaknesses: more sample size is needed

data[data['date'] == '2019-01-29']

##how many snow pictures would i need to label ?? 
## I think you would have to rerun the sort and match it with the data table where MODIS is non-null.

### save this ##
#data.to_csv('drive/My Drive/CBRE/Analysis/mergedData_allDataAug30.csv')

## add a binomial column for binomial logistic regression
## initiate the column. I am setting it to 0 because if it is set as empty, 
## it makes everything objects. It is risky to make everything 0, but as long as 
## you see some 1s, you know it worked. 

data['SnowCoverBinomial_1'] = 0 #""
data['SnowCoverBinomial_2'] = 0 #""
 

for i in range(0, len(data['SnowCover'])):
  if data['SnowCover'][i] >= 1: data['SnowCoverBinomial_1'][i] = 1.0
  else: data['SnowCoverBinomial_1'][i] = 0.0

for i in range(0, len(data['SnowCover'])):
  if data['SnowCover'][i] >= 2: data['SnowCoverBinomial_2'][i] = 1.0
  else: data['SnowCoverBinomial_2'][i] = 0.0


print(data)
#data.to_csv('drive/My Drive/CBRE/Analysis/mergedData_allData_timeZone.csv') ### this is what we want to work with in R


## print the differences in 1s in both options:
print(len(data[data['SnowCoverBinomial_1']== 1.0])) # 2083
print(len(data[data['SnowCoverBinomial_2']== 1.0])) # 2035

## so this final data frame looks good, except that James labeled a lot of 0s 
## when they should actually be 1s, 2s, 3s, or 4s. We will drop those, because 
## we need to go back and double what the deal is

data_woJamesZeros = data[~((data['Analyst'] == 'James Swartwood') & (data['SnowCover'] == 0.0))]
#data_woJamesZeros

### check for normality

plt.hist(data['NDSImodis'])
print(plt.show())

plt.hist(data['SnowCover'])
print(plt.show())

plt.hist(data_woJamesZeros['SnowCover'])
print(plt.show())

## conclusion: data is non-normal

def linearRegressionPlot(data):
  data.columns ## column names
### basic plot 
# basic plot 
## Scatter plot on Result5 
  fig = plt.figure(figsize=(12,5))
  ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes, i am not sure what these numbers do, but it works when they are together 
  plt.scatter(data['SnowCover'],data['NDSImodis']) 
  ax.set_xticks([0,1,2,3,4])
  ax.set_yticks([0,20,40,60,80,100])
  ax.set_ylabel('MODIS NDSI Snow Cover')
  ax.set_xlabel('Snow Classification')
  ## add line of best fit

  ## do statistical correlation 
  ## Because this violates the law of normality, we will do an ordinal linear regression test

  return plt.show() #, statistical test

plot1 = linearRegressionPlot(data)
plot2 = linearRegressionPlot(data_woJamesZeros) ## night time labeling is really hard

import seaborn as sns
import matplotlib.pyplot as plt

def seabornPlot(data):
  sns.set(style="white")
  ax = sns.boxplot(x='SnowCover', y='NDSImodis', data=data, showfliers = False)
  ax = sns.stripplot(x='SnowCover', y='NDSImodis', data=data, color=".40")
  sns.set(rc={'figure.figsize':(10,5)})
  # Set x-axis label
  plt.xlabel('Snow Classification')
  # Set y-axis label
  plt.ylabel('NDSI Snow Cover')

  return plt.show()

plot3 = seabornPlot(data)
plot4 = seabornPlot(data_woJamesZeros)

## ordinal regression analysis in python 

## We will use a pandas function CategoricalDtype to convert our 0,1,2,3,4 to an ordered category
#(data['SnowCover'].astype("category")).dtype
#(data['SnowCover'].astype("category")
## We will then use OrderedModel to understand the the ordered relationship between the two


from pandas.api.types import CategoricalDtype

categories = CategoricalDtype(categories=[0, 1, 2, 3, 4], ordered=True)
mod_log = OrderedModel(data['NDSImodis'],data['SnowCover'].astype(categories),distr='logit')

res_log = mod_log.fit(method='bfgs', disp=False)
res_log.summary()

### Coefficient: 0.7845

## see if changes for James's data removed 

## ordinal regression analysis in python 

## We will use a pandas function CategoricalDtype to convert our 0,1,2,3,4 to an ordered category
## We will then use OrderedModel to understand the the ordered relationship between the two

from pandas.api.types import CategoricalDtype

categories = CategoricalDtype(categories=[0, 1, 2, 3, 4], ordered=True)
mod_log = OrderedModel(data_woJamesZeros['NDSImodis'],data_woJamesZeros['SnowCover'].astype(categories),distr='logit')

res_log_woJames = mod_log.fit(method='bfgs', disp=False)
res_log_woJames.summary() 
# coefficient 1.5224

data

def seabornPlot_scatter(data, BinomialSetting):
  sns.set(style="white")
  ax = sns.scatterplot(data['NDSImodis'], data[BinomialSetting])

  ax = sns.regplot(x='NDSImodis', y=BinomialSetting, data=data,
                 logistic=True)
  ax.set_yticks([0,1])
  ax.set_xticks([-1,20,40,60,80,100])

  sns.set(rc={'figure.figsize':(10,5)})
  # Set x-axis label
  plt.xlabel('NDSI Snow Cover')
  # Set y-axis label
  plt.ylabel('Snow Classification')

  return plt.show()

plot5 = seabornPlot_scatter(data, 'SnowCoverBinomial_1')
plot6 = seabornPlot_scatter(data, 'SnowCoverBinomial_2')

snow_mod1 = logit("SnowCoverBinomial_1 ~ NDSImodis", data).fit()
print('SnowCoverBinomial_1',snow_mod1.summary())

snow_mod2 = logit("SnowCoverBinomial_2 ~ NDSImodis", data).fit()
print('SnowCoverBinomial_2',snow_mod2.summary())

"""The next step is to use simple correlation analysis...and see how well are they related using the
 correlation coefficient (r). If the r is small your conclusion would be that they are weakly related 
 and so no desirable comparisons and a larger value if r would suggest good comparisons s between the two series. 
 The third step where there is good correlation is to test the statistical significance of the r. Here you can use the
  Shapiro Welch test which would assume the two series are normally distributed (null hypothesis ) or not (alternative hypothesis). 
There are other tests you can do but let me hope my answer helps.

"""

## Regression

snow_mod = logit("Snow ~ NDSImodis + treecanopyheight + landclass"
                   "+ NDVImodis", fullData).fit()

print(snow_mod.summary())

import numpy as np

def photoCount2018(df):
  df['Date'] = pd.to_datetime(df['Date']) 
  CameraID = df["File"].str.split("_", n = 1, expand = True) 
  df['LokalitetID'] = CameraID[0].astype(int)
  df['SnowCover'].replace('', np.nan, inplace=True)
  df.dropna(subset=['SnowCover'], inplace=True)
  df.reset_index()
  ### since we only did the timelapse images we need to filter out the empty values

  return df

photoCount2018data = photoCount2018(df=CameraData) 
photoCount2018data = photoCount2018data.reset_index()


January = []
February = []
March = []
April = []
May = []
June = []
July = []
August = []
September = []
October = []
November = []
December = []

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 1:
    January.append(1)
  else: 
    January.append(0)

print(sum(January))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 2:
    February.append(1)
  else: 
    February.append(0)

print(sum(February))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 3:
    March.append(1)
  else: 
    March.append(0)

print(sum(March))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 4:
    April.append(1)
  else: 
    April.append(0)

print(sum(April))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 5:
    May.append(1)
  else: 
    May.append(0)

print(sum(May))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 6:
    June.append(1)
  else: 
    June.append(0)

print(sum(June))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 7:
    July.append(1)
  else: 
    July.append(0)

print(sum(July))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 8:
    August.append(1)
  else: 
    August.append(0)

print(sum(August))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 9:
    September.append(1)
  else: 
    September.append(0)

print(sum(September))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 10:
    October.append(1)
  else: 
    October.append(0)

print(sum(October))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 11:
    November.append(1)
  else: 
    November.append(0)

print(sum(November))

for i in range(0,len(photoCount2018data["Date"])):
  if photoCount2018data["Date"][i].month == 12:
    December.append(1)
  else: 
    December.append(0)

print(sum(December))

#
#502
#690
#1645
#854
#388
#359
#544
#10
#0
#0
#7
#22
#

502+690+1645+854+388+359+544+10+7+22
